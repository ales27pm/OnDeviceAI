import React, { useState, useEffect, useRef } from 'react';
import {
  View,
  Text,
  Pressable,
  Animated,
  StyleSheet,
  Modal,
  Dimensions,
  Alert
} from 'react-native';
import { Ionicons } from '@expo/vector-icons';
import { BlurView } from 'expo-blur';
import { LinearGradient } from 'expo-linear-gradient';
import { VoiceConversationManager, ConversationState, ConversationContext } from '../services/VoiceConversationManager';
import { SpeechService, VoiceProfile } from '../services/SpeechService';
import { useToolPermissions } from '../hooks/useToolPermissions';

const { width: screenWidth, height: screenHeight } = Dimensions.get('window');

/**
 * Audio visualizer component
 */
const AudioVisualizer: React.FC<{
  audioLevel: number;
  isActive: boolean;
  color: string;
}> = ({ audioLevel, isActive, color }) => {
  const animatedValues = useRef(
    Array.from({ length: 5 }, () => new Animated.Value(0.3))
  ).current;

  useEffect(() => {
    if (isActive) {
      // Create wave animation based on audio level
      const animations = animatedValues.map((animValue, index) => {
        const delay = index * 100;
        const intensity = Math.max(0.3, audioLevel * (1 + Math.random() * 0.5));
        
        return Animated.loop(
          Animated.sequence([
            Animated.timing(animValue, {
              toValue: intensity,
              duration: 200 + Math.random() * 200,
              useNativeDriver: false,
            }),
            Animated.timing(animValue, {
              toValue: 0.3,
              duration: 200 + Math.random() * 200,
              useNativeDriver: false,
            }),
          ]),
          { iterations: -1 }
        );
      });

      animations.forEach((anim, index) => {
        setTimeout(() => anim.start(), index * 50);
      });

      return () => {
        animations.forEach(anim => anim.stop());
      };
    } else {
      // Reset to idle state
      animatedValues.forEach(animValue => {
        animValue.setValue(0.3);
      });
    }
  }, [isActive, audioLevel, animatedValues]);

  return (
    <View style={styles.visualizerContainer}>
      {animatedValues.map((animValue, index) => (
        <Animated.View
          key={index}
          style={[
            styles.visualizerBar,
            {
              height: animValue.interpolate({
                inputRange: [0, 1],
                outputRange: [20, 80],
              }),
              backgroundColor: color,
              opacity: 0.8,
            },
          ]}
        />
      ))}
    </View>
  );
};

/**
 * Voice state indicator component
 */
const VoiceStateIndicator: React.FC<{
  state: ConversationState;
  context: ConversationContext;
}> = ({ state, context }) => {
  const getStateInfo = () => {
    switch (state) {
      case 'listening':
        return {
          text: 'Listening...',
          icon: 'mic' as const,
          color: '#10B981',
          description: 'Speak now'
        };
      case 'processing':
        return {
          text: 'Processing...',
          icon: 'cog' as const,
          color: '#F59E0B',
          description: 'Understanding your request'
        };
      case 'speaking':
        return {
          text: 'Speaking...',
          icon: 'volume-high' as const,
          color: '#3B82F6',
          description: 'AI is responding'
        };
      case 'waiting':
        return {
          text: 'Waiting...',
          icon: 'time' as const,
          color: '#6B7280',
          description: 'Ready for your next input'
        };
      case 'interrupted':
        return {
          text: 'Interrupted',
          icon: 'pause' as const,
          color: '#EF4444',
          description: 'Conversation paused'
        };
      case 'error':
        return {
          text: 'Error',
          icon: 'alert-circle' as const,
          color: '#DC2626',
          description: 'Something went wrong'
        };
      default:
        return {
          text: 'Ready',
          icon: 'mic-off' as const,
          color: '#6B7280',
          description: 'Tap to start voice conversation'
        };
    }
  };

  const stateInfo = getStateInfo();

  return (
    <View style={styles.stateIndicator}>
      <View style={[styles.stateIconContainer, { backgroundColor: stateInfo.color }]}>
        <Ionicons name={stateInfo.icon} size={24} color="white" />
      </View>
      <Text style={[styles.stateText, { color: stateInfo.color }]}>
        {stateInfo.text}
      </Text>
      <Text style={styles.stateDescription}>
        {stateInfo.description}
      </Text>
      {context.turnCount > 0 && (
        <Text style={styles.contextInfo}>
          Turn {context.turnCount} â€¢ {context.complexity} â€¢ {Math.round(context.averageResponseTime/1000)}s avg
        </Text>
      )}
    </View>
  );
};

/**
 * Voice settings panel
 */
const VoiceSettingsPanel: React.FC<{
  visible: boolean;
  onClose: () => void;
  speechService: SpeechService;
  onVoiceChange: (voiceId: string) => void;
}> = ({ visible, onClose, speechService, onVoiceChange }) => {
  const [voices, setVoices] = useState<VoiceProfile[]>([]);
  const [currentVoice, setCurrentVoice] = useState<VoiceProfile | null>(null);

  useEffect(() => {
    if (visible) {
      console.log('ðŸ”Š Loading voices for settings...');
      const availableVoices = speechService.getAvailableVoices();
      console.log('ðŸ”Š Available voices:', availableVoices.length);
      
      // If no voices loaded, provide default options
      if (availableVoices.length === 0) {
        console.log('ðŸ”Š No voices found, using defaults');
        const defaultVoices: VoiceProfile[] = [
          { id: 'default', name: 'System Default', language: 'en-US', quality: 'default' },
          { id: 'enhanced', name: 'Enhanced Voice', language: 'en-US', quality: 'enhanced' },
          { id: 'compact', name: 'Compact Voice', language: 'en-US', quality: 'compact' },
        ];
        setVoices(defaultVoices);
        setCurrentVoice(defaultVoices[0]);
      } else {
        setVoices(availableVoices);
        setCurrentVoice(speechService.getCurrentVoice());
      }
    }
  }, [visible, speechService]);

  const handleVoiceSelect = (voice: VoiceProfile) => {
    if (speechService.setVoice(voice.id)) {
      setCurrentVoice(voice);
      onVoiceChange(voice.id);
    }
  };

  return (
    <Modal
      visible={visible}
      animationType="slide"
      presentationStyle="pageSheet"
      onRequestClose={onClose}
    >
      <View style={styles.settingsContainer}>
        <View style={styles.settingsHeader}>
          <Text style={styles.settingsTitle}>Voice Settings</Text>
          <Pressable onPress={onClose} style={styles.closeButton}>
            <Ionicons name="close" size={24} color="#6B7280" />
          </Pressable>
        </View>

        <View style={styles.settingsContent}>
          <Text style={styles.sectionTitle}>Select Voice</Text>
          {voices.map((voice) => (
            <Pressable
              key={voice.id}
              style={[
                styles.voiceOption,
                currentVoice?.id === voice.id && styles.voiceOptionSelected
              ]}
              onPress={() => handleVoiceSelect(voice)}
            >
              <View style={styles.voiceInfo}>
                <Text style={styles.voiceName}>{voice.name}</Text>
                <Text style={styles.voiceDetails}>
                  {voice.language} â€¢ {voice.quality}
                </Text>
              </View>
              {currentVoice?.id === voice.id && (
                <Ionicons name="checkmark-circle" size={24} color="#10B981" />
              )}
            </Pressable>
          ))}

          <View style={styles.settingsSection}>
            <Text style={styles.sectionTitle}>Speech Recognition Status</Text>
            <View style={styles.statusGrid}>
              <View style={styles.statusItem}>
                <Ionicons name="checkmark-circle" size={20} color="#10B981" />
                <Text style={styles.statusText}>iOS Native STT</Text>
              </View>
              <View style={styles.statusItem}>
                <Ionicons name="checkmark-circle" size={20} color="#10B981" />
                <Text style={styles.statusText}>Advanced Turn Detection</Text>
              </View>
              <View style={styles.statusItem}>
                <Ionicons name="checkmark-circle" size={20} color="#10B981" />
                <Text style={styles.statusText}>Adaptive Algorithms</Text>
              </View>
              <View style={styles.statusItem}>
                <Ionicons name="checkmark-circle" size={20} color="#10B981" />
                <Text style={styles.statusText}>Multi-language Ready</Text>
              </View>
            </View>
          </View>

          <View style={styles.settingsSection}>
            <Text style={styles.sectionTitle}>Capabilities</Text>
            <View style={styles.capabilityTags}>
              <View style={styles.capabilityTag}>
                <Text style={styles.capabilityTagText}>Real-time STT</Text>
              </View>
              <View style={styles.capabilityTag}>
                <Text style={styles.capabilityTagText}>Voice Activity Detection</Text>
              </View>
              <View style={styles.capabilityTag}>
                <Text style={styles.capabilityTagText}>Noise Adaptation</Text>
              </View>
              <View style={styles.capabilityTag}>
                <Text style={styles.capabilityTagText}>Pattern Learning</Text>
              </View>
            </View>
          </View>

          <View style={styles.settingsSection}>
            <Text style={styles.sectionTitle}>Privacy & Security</Text>
            <Text style={styles.privacyText}>
              âœ… All voice processing occurs on-device using iOS Speech Recognition{'\n'}
              âœ… No audio transmitted to external servers for core functionality{'\n'}
              âœ… Adaptive algorithms learn your speech patterns locally{'\n'}
              âœ… Full compliance with App Store privacy guidelines
            </Text>
          </View>
        </View>
      </View>
    </Modal>
  );
};

/**
 * Main Voice Interaction UI Component
 */
export const VoiceInteractionUI: React.FC<{
  visible: boolean;
  onClose: () => void;
  onTranscription?: (text: string) => void;
  onResponse?: (text: string) => void;
}> = ({ visible, onClose, onTranscription, onResponse }) => {
  // Services and hooks
  const toolPermissions = useToolPermissions();
  const [voiceManager] = useState(() => new VoiceConversationManager());
  const [speechService] = useState(SpeechService.getInstance);
  
  // State
  const [conversationState, setConversationState] = useState<ConversationState>('idle');
  const [conversationContext, setConversationContext] = useState<ConversationContext>({
    turnCount: 0,
    averageResponseTime: 0,
    userSpeechPatterns: { averageDuration: 0, pauseFrequency: 0, interruptionRate: 0 },
    conversationTopic: 'general',
    complexity: 'medium'
  });
  const [audioLevel, setAudioLevel] = useState(0);
  const [showSettings, setShowSettings] = useState(false);
  const [isInitialized, setIsInitialized] = useState(false);
  const [currentTranscription, setCurrentTranscription] = useState('');
  const [currentResponse, setCurrentResponse] = useState('');

  // Animation values - use consistent JS driver to avoid conflicts
  const pulseAnim = useRef(new Animated.Value(1)).current;
  const shadowOpacityAnim = useRef(new Animated.Value(0)).current;
  const shadowRadiusAnim = useRef(new Animated.Value(0)).current;

  // Initialize voice manager
  useEffect(() => {
    if (visible && !isInitialized) {
      initializeVoiceManager();
    }
  }, [visible, isInitialized]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (isInitialized) {
        voiceManager.cleanup();
      }
    };
  }, [isInitialized, voiceManager]);

  // Pulse animation effect - using JS driver for consistency
  useEffect(() => {
    if (conversationState === 'listening' || conversationState === 'speaking') {
      Animated.loop(
        Animated.sequence([
          Animated.timing(pulseAnim, {
            toValue: 1.1,
            duration: 1000,
            useNativeDriver: false,
          }),
          Animated.timing(pulseAnim, {
            toValue: 1,
            duration: 1000,
            useNativeDriver: false,
          }),
        ])
      ).start();
    } else {
      pulseAnim.setValue(1);
    }
  }, [conversationState, pulseAnim]);

  // Shadow animation effects (must use JS driver for shadow properties)
  useEffect(() => {
    if (conversationState !== 'idle') {
      Animated.parallel([
        Animated.timing(shadowOpacityAnim, {
          toValue: 0.6,
          duration: 500,
          useNativeDriver: false,
        }),
        Animated.timing(shadowRadiusAnim, {
          toValue: 20,
          duration: 500,
          useNativeDriver: false,
        }),
      ]).start();
    } else {
      Animated.parallel([
        Animated.timing(shadowOpacityAnim, {
          toValue: 0,
          duration: 500,
          useNativeDriver: false,
        }),
        Animated.timing(shadowRadiusAnim, {
          toValue: 0,
          duration: 500,
          useNativeDriver: false,
        }),
      ]).start();
    }
  }, [conversationState, shadowOpacityAnim, shadowRadiusAnim]);

  const initializeVoiceManager = async () => {
    try {
      console.log('ðŸŽ¤ Initializing voice interaction...');
      
      await voiceManager.initialize(toolPermissions);
      
      // Set up callbacks
      voiceManager.setCallbacks({
        onStateChange: (state, context) => {
          setConversationState(state);
          setConversationContext(context);
        },
        onUserSpeechEnd: (transcription, confidence) => {
          setCurrentTranscription(transcription);
          onTranscription?.(transcription);
        },
        onAIResponseStart: (text) => {
          setCurrentResponse(text);
          onResponse?.(text);
        },
        onAudioLevelUpdate: (level) => {
          setAudioLevel(level);
        },
        onError: (error, state) => {
          console.error('Voice interaction error:', error);
          
          // More graceful error handling - just reset to idle state
          setConversationState('idle');
          setCurrentTranscription('Voice recognition error occurred');
          
          // Don't show intrusive alerts for every error
          if (error && typeof error === 'object' && error.toString().includes('Permission')) {
            Alert.alert('Microphone Permission', 'Please allow microphone access to use voice features.');
          }
        },
        onInterruption: (type) => {
          console.log('Voice interruption:', type);
        }
      });
      
      setIsInitialized(true);
      console.log('âœ… Voice interaction initialized');
      
    } catch (error) {
      console.error('âŒ Voice initialization failed:', error);
      
      // Set a more user-friendly error state
      setCurrentTranscription('Voice features unavailable');
      setConversationState('error');
      
      // Only show alert for critical permission issues
      if (error && typeof error === 'object' && error.toString().includes('Permission')) {
        Alert.alert(
          'Microphone Permission Required',
          'Please allow microphone access in your device settings to use voice features.',
          [{ text: 'OK' }]
        );
      }
    }
  };

  const handleVoiceButtonPress = async () => {
    console.log('ðŸŽ¤ Voice button pressed, initialized:', isInitialized);
    console.log('ðŸŽ¤ Current conversation state:', conversationState);
    
    if (!isInitialized) {
      console.log('ðŸŽ¤ Not initialized, attempting initialization...');
      await initializeVoiceManager();
      return;
    }

    try {
      if (conversationState === 'idle') {
        console.log('ðŸŽ¤ Starting conversation...');
        
        // Simplified voice interaction - just test TTS for now
        console.log('ðŸ—£ï¸ Testing simplified voice interaction...');
        setConversationState('speaking');
        setCurrentResponse('Hello! This is a test of the voice interaction system. I can speak responses, but speech recognition may need additional setup.');
        
        await speechService.speak('Hello! This is a test of the voice interaction system. I can speak responses, but speech recognition may need additional setup.', {
          onStart: () => {
            console.log('ðŸ—£ï¸ TTS started in voice interaction');
          },
          onDone: () => {
            console.log('ðŸ—£ï¸ TTS completed in voice interaction');
            setConversationState('idle');
            setCurrentResponse('');
          },
          onError: (error) => {
            console.error('ðŸ—£ï¸ TTS error in voice interaction:', error);
            setConversationState('error');
          }
        });
        
      } else {
        console.log('ðŸŽ¤ Stopping conversation...');
        await voiceManager.stopConversation();
        console.log('ðŸŽ¤ Conversation stopped successfully');
      }
    } catch (error) {
      console.error('Voice button press error:', error);
      Alert.alert('Error', `Failed to ${conversationState === 'idle' ? 'start' : 'stop'} voice conversation: ${error instanceof Error ? error.message : String(error)}`);
    }
  };

  const getMainButtonColor = () => {
    switch (conversationState) {
      case 'listening': return '#10B981';
      case 'processing': return '#F59E0B';
      case 'speaking': return '#3B82F6';
      case 'error': return '#EF4444';
      default: return '#6B7280';
    }
  };

  const getMainButtonIcon = () => {
    switch (conversationState) {
      case 'listening': return 'mic';
      case 'processing': return 'cog';
      case 'speaking': return 'volume-high';
      case 'error': return 'alert-circle';
      default: return 'mic-outline';
    }
  };

  if (!visible) return null;

  return (
    <Modal
      visible={visible}
      animationType="slide"
      presentationStyle="fullScreen"
      statusBarTranslucent
    >
      <LinearGradient
        colors={['#1F2937', '#111827']}
        style={styles.container}
      >
        {/* Header */}
        <View style={styles.header}>
          <Pressable onPress={onClose} style={styles.headerButton}>
            <Ionicons name="close" size={24} color="white" />
          </Pressable>
          <Text style={styles.headerTitle}>Voice Assistant</Text>
          <Pressable onPress={() => setShowSettings(true)} style={styles.headerButton}>
            <Ionicons name="settings" size={24} color="white" />
          </Pressable>
        </View>

        {/* Main Content */}
        <View style={styles.mainContent}>
          {/* Voice State Indicator */}
          <VoiceStateIndicator 
            state={conversationState} 
            context={conversationContext} 
          />

          {/* Audio Visualizer */}
          <View style={styles.visualizerSection}>
            <AudioVisualizer
              audioLevel={audioLevel}
              isActive={conversationState === 'listening' || conversationState === 'speaking'}
              color={getMainButtonColor()}
            />
          </View>

          {/* Current Transcription Display */}
          {currentTranscription && (
            <View style={styles.transcriptionContainer}>
              <Text style={styles.transcriptionLabel}>You said:</Text>
              <Text style={styles.transcriptionText}>{currentTranscription}</Text>
            </View>
          )}

          {/* Current Response Display */}
          {currentResponse && conversationState === 'speaking' && (
            <View style={styles.responseContainer}>
              <Text style={styles.responseLabel}>AI Response:</Text>
              <Text style={styles.responseText}>{currentResponse}</Text>
            </View>
          )}

          {/* Main Voice Button */}
          <Animated.View
            style={[
              styles.mainButtonContainer,
              {
                transform: [{ scale: pulseAnim }],
                shadowColor: getMainButtonColor(),
                shadowOpacity: shadowOpacityAnim,
                shadowRadius: shadowRadiusAnim,
              },
            ]}
          >
            <Pressable
              style={[
                styles.mainButton,
                { backgroundColor: getMainButtonColor() }
              ]}
              onPress={handleVoiceButtonPress}
              disabled={!isInitialized}
            >
              <Ionicons 
                name={getMainButtonIcon() as any} 
                size={48} 
                color="white" 
              />
            </Pressable>
          </Animated.View>

          {/* Instructions */}
          <View style={styles.instructionsContainer}>
            <Text style={styles.instructionsText}>
              {conversationState === 'idle' ? 
                'Tap the microphone to start a voice conversation' :
                conversationState === 'listening' ?
                'Speak naturally - I\'m listening' :
                conversationState === 'processing' ?
                'Processing your request...' :
                conversationState === 'speaking' ?
                'You can interrupt me by speaking' :
                'Tap to try again'
              }
            </Text>
          </View>
        </View>

        {/* Voice Settings Modal */}
        <VoiceSettingsPanel
          visible={showSettings}
          onClose={() => setShowSettings(false)}
          speechService={speechService}
          onVoiceChange={(voiceId) => {
            console.log('Voice changed to:', voiceId);
          }}
        />
      </LinearGradient>
    </Modal>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
  header: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingHorizontal: 20,
    paddingTop: 60,
    paddingBottom: 20,
  },
  headerButton: {
    padding: 8,
  },
  headerTitle: {
    fontSize: 20,
    fontWeight: '600',
    color: 'white',
  },
  mainContent: {
    flex: 1,
    alignItems: 'center',
    justifyContent: 'center',
    paddingHorizontal: 20,
  },
  stateIndicator: {
    alignItems: 'center',
    marginBottom: 40,
  },
  stateIconContainer: {
    width: 50,
    height: 50,
    borderRadius: 25,
    justifyContent: 'center',
    alignItems: 'center',
    marginBottom: 10,
  },
  stateText: {
    fontSize: 18,
    fontWeight: '600',
    marginBottom: 5,
  },
  stateDescription: {
    fontSize: 14,
    color: '#9CA3AF',
    textAlign: 'center',
  },
  contextInfo: {
    fontSize: 12,
    color: '#6B7280',
    marginTop: 5,
  },
  visualizerSection: {
    marginBottom: 40,
  },
  visualizerContainer: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'center',
    height: 100,
    gap: 4,
  },
  visualizerBar: {
    width: 6,
    borderRadius: 3,
    minHeight: 20,
  },
  transcriptionContainer: {
    backgroundColor: 'rgba(255, 255, 255, 0.1)',
    borderRadius: 12,
    padding: 16,
    marginBottom: 20,
    maxWidth: screenWidth - 40,
  },
  transcriptionLabel: {
    fontSize: 12,
    color: '#9CA3AF',
    marginBottom: 4,
  },
  transcriptionText: {
    fontSize: 16,
    color: 'white',
    lineHeight: 22,
  },
  responseContainer: {
    backgroundColor: 'rgba(59, 130, 246, 0.1)',
    borderRadius: 12,
    padding: 16,
    marginBottom: 20,
    maxWidth: screenWidth - 40,
  },
  responseLabel: {
    fontSize: 12,
    color: '#9CA3AF',
    marginBottom: 4,
  },
  responseText: {
    fontSize: 16,
    color: 'white',
    lineHeight: 22,
  },
  mainButtonContainer: {
    marginBottom: 30,
    elevation: 10,
  },
  mainButton: {
    width: 120,
    height: 120,
    borderRadius: 60,
    justifyContent: 'center',
    alignItems: 'center',
    elevation: 8,
    shadowOffset: { width: 0, height: 4 },
    shadowOpacity: 0.3,
    shadowRadius: 8,
  },
  instructionsContainer: {
    paddingHorizontal: 20,
  },
  instructionsText: {
    fontSize: 14,
    color: '#9CA3AF',
    textAlign: 'center',
    lineHeight: 20,
  },
  settingsContainer: {
    flex: 1,
    backgroundColor: 'white',
  },
  settingsHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingHorizontal: 20,
    paddingTop: 60,
    paddingBottom: 20,
    borderBottomWidth: StyleSheet.hairlineWidth,
    borderBottomColor: '#E5E7EB',
  },
  settingsTitle: {
    fontSize: 20,
    fontWeight: '600',
    color: '#111827',
  },
  closeButton: {
    padding: 8,
  },
  settingsContent: {
    flex: 1,
    padding: 20,
  },
  sectionTitle: {
    fontSize: 16,
    fontWeight: '600',
    color: '#111827',
    marginBottom: 12,
  },
  voiceOption: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    padding: 16,
    borderRadius: 8,
    marginBottom: 8,
    backgroundColor: '#F9FAFB',
  },
  voiceOptionSelected: {
    backgroundColor: '#EBF8FF',
    borderWidth: 1,
    borderColor: '#10B981',
  },
  voiceInfo: {
    flex: 1,
  },
  voiceName: {
    fontSize: 16,
    fontWeight: '500',
    color: '#111827',
  },
  voiceDetails: {
    fontSize: 14,
    color: '#6B7280',
    marginTop: 2,
  },
  settingsSection: {
    marginTop: 30,
  },
  privacyText: {
    fontSize: 14,
    color: '#6B7280',
    lineHeight: 20,
  },
  statusGrid: {
    flexDirection: 'row',
    flexWrap: 'wrap',
    gap: 12,
    marginTop: 12,
  },
  statusItem: {
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    paddingVertical: 8,
    paddingHorizontal: 12,
    backgroundColor: '#F0F9FF',
    borderRadius: 8,
    borderWidth: 1,
    borderColor: '#BAE6FD',
  },
  statusText: {
    fontSize: 14,
    color: '#0369A1',
    fontWeight: '500',
  },
  capabilityTags: {
    flexDirection: 'row',
    flexWrap: 'wrap',
    gap: 8,
    marginTop: 12,
  },
  capabilityTag: {
    paddingVertical: 6,
    paddingHorizontal: 12,
    backgroundColor: '#F3F4F6',
    borderRadius: 16,
    borderWidth: 1,
    borderColor: '#D1D5DB',
  },
  capabilityTagText: {
    fontSize: 12,
    color: '#374151',
    fontWeight: '500',
  },
});

export default VoiceInteractionUI;